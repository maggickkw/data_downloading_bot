{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruatl90EltEu",
        "outputId": "04a00bfd-ec50-45f9-f57d-5610d9ca5e50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing CIK: 0001326205\n",
            "Processing CIK: 0001326190\n",
            "Processing CIK: 0001723648\n",
            "Processing CIK: 0000811222Processing CIK: 0001246713\n",
            "\n",
            "Processing CIK: 0001298663\n",
            "Processing CIK: 0001320716\n",
            "Processing CIK: 0001331858\n",
            "Processing CIK: 0001374346\n",
            "Saved header data to header_data.xlsx\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import os\n",
        "import pandas as pd\n",
        "import concurrent.futures\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# Constants\n",
        "BASE_URL = \"https://www.sec.gov\"\n",
        "SEARCH_URL = \"https://www.sec.gov/cgi-bin/browse-edgar\"\n",
        "HEADERS = {\n",
        "    'Host': 'www.sec.gov',\n",
        "    'Connection': 'close',\n",
        "    'Accept': 'application/json, text/javascript, */*; q=0.01',\n",
        "    'X-Requested-With': 'XMLHttpRequest',\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36',\n",
        "}\n",
        "OUTPUT_DIR = \"Successful\"\n",
        "LOG_FILE = \"edgar_download_log.txt\"\n",
        "EXCEL_FILE = \"header_data.xlsx\"\n",
        "\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "    os.makedirs(OUTPUT_DIR)\n",
        "\n",
        "\n",
        "def load_ciks(file_path):\n",
        "    try:\n",
        "        cik_data = pd.read_excel(file_path, sheet_name=None)\n",
        "        ciks_df = cik_data[list(cik_data.keys())[0]]['cik']\n",
        "        cleaned_ciks = ciks_df.apply(lambda x: x.replace('CIK', '').strip())\n",
        "        return cleaned_ciks.tolist()\n",
        "    except Exception as e:\n",
        "        log_message(f\"Error loading CIKs from file {file_path}: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def fetch_html(url):\n",
        "    try:\n",
        "        response = requests.get(url, headers=HEADERS)\n",
        "        response.raise_for_status()\n",
        "        return response.text\n",
        "    except requests.RequestException as e:\n",
        "        log_message(f\"Failed to fetch {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def log_message(message):\n",
        "    with open(LOG_FILE, 'a') as log_file:\n",
        "        log_file.write(message + '\\n')\n",
        "    print(message)\n",
        "\n",
        "\n",
        "def parse_filings(cik):\n",
        "    url = f\"{SEARCH_URL}?CIK={cik}&action=getcompany&type=S-1&S-1/A\"\n",
        "    html = fetch_html(url)\n",
        "    if html:\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "        filings = []\n",
        "        table = soup.find('table', class_='tableFile2')\n",
        "        if table:\n",
        "            rows = table.find_all('tr')\n",
        "            for row in rows[1:]:\n",
        "                cells = row.find_all('td')\n",
        "                if cells:\n",
        "                    form_type = cells[0].text.strip()\n",
        "                    if form_type in [\"S-1\", \"S-1/A\"]:\n",
        "                        filing_href = BASE_URL + cells[1].a['href']\n",
        "                        filing_date = cells[3].text.strip()\n",
        "                        filings.append((filing_date, filing_href))\n",
        "        return filings\n",
        "    return []\n",
        "\n",
        "\n",
        "def download_and_check_filing(filing_href, cik, header_data_list):\n",
        "    doc_url = filing_href.replace(\"-index.htm\", \".txt\")\n",
        "    response = requests.get(doc_url, headers=HEADERS)\n",
        "    if response.status_code == 200:\n",
        "        text_content = response.text\n",
        "        header_data = extract_header_data(text_content)\n",
        "        if header_data:\n",
        "            header_data_list.append(header_data)\n",
        "            file_name = f\"{header_data['CIK']}_{header_data['FILED_AS_OF_DATE']}_{header_data['COMPANY_CONFORMED_NAME']}_{header_data['FORM_TYPE']}_{header_data['SEC_FILE_NUMBER']}_{header_data['FILM_NUMBER']}.txt\"\n",
        "            file_name = file_name.replace(\" \", \"_\")\n",
        "            save_path = os.path.join(OUTPUT_DIR, file_name)\n",
        "            with open(save_path, 'wb') as file:\n",
        "                file.write(response.content)\n",
        "            log_message(f\"Downloaded and saved as: {file_name}\")\n",
        "        else:\n",
        "            log_message(f\"Failed to extract header data from {doc_url}\")\n",
        "    else:\n",
        "        log_message(f\"Failed to download {doc_url}\")\n",
        "\n",
        "\n",
        "def extract_header_data(text_content):\n",
        "    header_data = {}\n",
        "    patterns = {\n",
        "        'CIK': re.compile(r\"CENTRAL INDEX KEY:\\s+(\\d+)\"),\n",
        "        'FILED_AS_OF_DATE': re.compile(r\"FILED AS OF DATE:\\s+(\\d+)\"),\n",
        "        'COMPANY_CONFORMED_NAME': re.compile(r\"COMPANY CONFORMED NAME:\\s+(.*?)\\n\"),\n",
        "        'FORM_TYPE': re.compile(r\"FORM TYPE:\\s+(.*?)\\n\"),\n",
        "        'SEC_FILE_NUMBER': re.compile(r\"SEC FILE NUMBER:\\s+(\\d+-\\d+)\"),\n",
        "        'FILM_NUMBER': re.compile(r\"FILM NUMBER:\\s+(\\d+)\")\n",
        "    }\n",
        "    for key, pattern in patterns.items():\n",
        "        match = pattern.search(text_content)\n",
        "        if match:\n",
        "            header_data[key] = match.group(1)\n",
        "    return header_data if len(header_data) == 6 else None\n",
        "\n",
        "\n",
        "def process_cik(cik, header_data_list):\n",
        "    log_message(f\"Processing CIK: {cik}\")\n",
        "    filings = parse_filings(cik)\n",
        "    if filings:\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
        "            futures = [executor.submit(download_and_check_filing, filing[1], cik, header_data_list) for filing in filings]\n",
        "            concurrent.futures.wait(futures)\n",
        "\n",
        "\n",
        "def save_to_excel(header_data_list):\n",
        "    df = pd.DataFrame(header_data_list)\n",
        "    df.columns = ['cik', 'filed', 'spac', 'form', 'file number', 'film number']\n",
        "    df.to_excel(EXCEL_FILE, index=False)\n",
        "    log_message(f\"Saved header data to {EXCEL_FILE}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    file_path = 'path to your data file'\n",
        "    ciks = load_ciks(file_path)\n",
        "    if ciks:\n",
        "        header_data_list = []\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
        "            futures = [executor.submit(process_cik, cik, header_data_list) for cik in ciks]\n",
        "            concurrent.futures.wait(futures)\n",
        "        save_to_excel(header_data_list)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}